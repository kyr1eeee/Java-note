### 数据库架构

![](C:\Users\kyrie\Pictures\database.PNG)

### 索引

#### B+树

B+树作为实现索引的数据结构的优势：

1. B+树中非叶子节点只存放索引，不存放实际的数据，因此固定大小的盘块能够存下更多的索引，树的高度也会降低，相应的查找的时候的IO次数也会少。
2. 查找效率更加稳定，因为非叶子节点只存索引，所有的查找都会落到叶子结点，查找的时间复杂度都是O(logn)
3. B+树叶子节点含有指向下一个叶子节点的指针，对于范围查询的操作，效率大大的提升。

##### Hash索引

效率高，等值查找（=，in）

但是其无法用于数据的排序操作

无法利用部分索引键来查询

不能避免表扫描，因为可能出现hash碰撞，还是要扫描一遍比较。

出现大量hash碰撞时，查询效率甚至比B+树索引低

### InnoDB&MyISAM

![](C:\Users\kyrie\Pictures\InnoDB&MyISAM.PNG)

* MyISAM默认表级锁，不支持行级锁，InnoDB默认行级锁，也支持表级锁。

普通select，加读锁（共享锁S）update,insert delete select for update 加排它锁（写锁X）

* MyISAM不支持事务，InnoDB支持事务。

InnoDB默认是自动提交

```java
show variables like "autocommit";

//InnoDB 普通的select为非阻塞select，
select * from table where id = '1' lock in share mode;//显示加读锁
```

不走索引的时候走表级锁，走索引才是行级锁。

* MyISAM适用的场景

频繁执行全表count，因为MyISAM 会为表记录表的字段数。

对数据的增删改频率低，查询操作频率高，因为增删改锁表，影响查询，

没有事务

* InnoDB的场景

增删改查比较频繁，以为只锁行嘛；

可靠性要求高，要求支持事务；

#### 锁分类

![](C:\Users\kyrie\Pictures\mysql锁的分类.PNG)

#### 当前读&快照读

![](C:\Users\kyrie\Pictures\当前读与快照读.PNG)

![为什么update等也是快照读](C:\Users\kyrie\Pictures\update等.PNG)

update的执行流程如上所示，会一次对每一行当前读，加排它锁，更新。。。

快照读：不加锁的非阻塞读，select。（注意前提是事务隔离级别不是serializable）

#### InnoDB  RR级别下如何防止幻读

表象：快照读，第一次快照读生成readview，之后的快照读都是在这个readview下，但其实可能新数据已经插入了。

根本原因是next-key锁： 行锁+gap锁（gap锁不支持在RC隔离级别以下）

##### gap锁

![](C:\Users\kyrie\Pictures\gap锁.PNG)

**主机索引和非唯一索引部分命中**则gap锁的范围为：

update ...where id in (5,8,11)//假设只有5，11命中，则5-11会加上gap锁

update ...where id =7//假设**没命中**，则在7的周围加间隙锁。（周围是根据行记录的值决定）

![](C:\Users\kyrie\Pictures\gap锁2.PNG)

**非唯一索引**

可以看到所有gap的范围，当执行上面的delete时，防止insert的gap锁就是在9的周围（6，11]

**不走索引**

直接锁定整张表的gap。

#### 如何优化慢查询

1. 根据慢日志定位慢查询sql

   开启慢查询，设置慢查询的查询时间

   ```java
   show variables like "quer%";//查看系统变量
   show status like "slow_queries";//查询慢查询的数量
   set global slow_query_log=on;
   set global long_query_time =5;
   //永久设置可以去my.cnf,my.ini修改
   ```

   

2. explain分析sql

   ![](C:\Users\kyrie\Pictures\explain_type.PNG)

   ![](C:\Users\kyrie\Pictures\explain_extra.PNG)

3. 优化sql，建索引等

#### 联合索引

* 最左匹配原则





#### MVCC

###### 三个隐士字段

事务ID（随创建时间自增），回滚指针（配合undolog形成版本链），自增主键（用户未指定主键时引擎创建）

###### undolog

insert undolog(insert 事务提交就可以删除了)

update/delete  (当purge线程的readview都对版本链的记录可见时，又pruge线程删除)

###### readview（维护一个List,即当前活跃的事务ID list）配合的还有up_limit_id（最小的ID）  low_limit_id（最大的ID+1）.

快照读时，根据readview遵从一个可见性算法。

即那版本链的头的事务ID去和up_limit_id对比，如果小于，那么版本链头的记录对当前事务肯定可见；

否则，再和low_limit_id对比，如果大于等于，那版本链头的记录对当前事务肯定不可见；

否则，继续与list中的事务ID对比，如果都不等，则可见，否则不可见。





#### binlog redolog undolog

参考：https://juejin.im/post/5c249e9a6fb9a049d5198dd5

https://www.jianshu.com/p/907f9002442e

https://www.cnblogs.com/kongzhongqijing/articles/7905051.html

**binlog**

即二进制日志,它记录了数据库上的所有改变，并以二进制的形式保存在磁盘中；它可以用来查看数据库的变更历史、数据库增量备份和恢复、Mysql的复制（主从数据库的复制）。语句以“事件”的形式保存，它描述数据更改。

因为有了数据更新的binlog，所以可以用于实时备份，与master/slave复制。高可用与数据恢复。

1.恢复使能够最大可能地更新数据库，因为二进制日志包含备份后进行的所有更新。
2.在主复制服务器上记录所有将发送给从服务器的语句。

**Redo log**
记录的是新数据的备份。在事务提交前，只要将Redo Log持久化即可，不需要将数据持久化。当系统崩溃时，虽然数据没有持久化，但是RedoLog已经持久化。系统可以根据RedoLog的内容，将所有数据恢复到最新的状态。

InnoDB有buffer pool（简称bp）。bp是数据库页面的缓存，对InnoDB的任何修改操作都会首先在bp的page上进行，然后这样的页面将被标记为dirty并被放到专门的flush list上，后续将由master thread或专门的刷脏线程阶段性的将这些页面写入磁盘（disk or ssd）。这样的好处是避免每次写操作都操作磁盘导致大量的随机IO，阶段性的刷脏可以将多次对页面的修改merge成一次IO操作，同时异步写入也降低了访问的时延。然而，如果在dirty page还未刷入磁盘时，server非正常关闭，这些修改操作将会丢失，如果写入操作正在进行，甚至会由于损坏数据文件导致数据库不可用。为了避免上述问题的发生，Innodb将所有对页面的修改操作写入一个专门的文件，并在数据库启动时从此文件进行恢复操作，这个文件就是redo log file。这样的技术推迟了bp页面的刷新，从而提升了数据库的吞吐，有效的降低了访问时延。带来的问题是额外的写redo log操作的开销（顺序IO，当然很快），以及数据库启动时恢复操作所需的时间。

**redolog** 重做日志，属于物理日志，上面存储的是**数据库中最终的内容**，有固定的大小。(一般设置innodb_flush_log_at_trx_commit为1，表示commit事务时将redolog上面的数据刷入到磁盘，具有两个状态分别是**prepare**和**commit**，在MySQL重启恢复时会根据commit状态恢复数据。)

**binlog** 归档日志，属于逻辑日志，上面存储的是最初的修改逻辑可以简单的理解为sql语句。（一般设置sync_binlog为1，表示commit事务时将binlog上面的数据刷入到磁盘进行归档。数据恢复和同步都是通过binlog来实现的）

下面以文字的方式再次描述一下**update T set value = value+1 where ID =2**的过程。

1. 词法分析器识别出事update语句；
2. 执行器去InnoDB中进行查询，找到满足ID = 2 的数据；
3. 执行器将value的值加1；
4. 执行器让InnoDB将刚刚的新值写入到InnoDB的内存中；
5. InnoDB在redolog中加入一条记录，并把该记录的状态设置为**prepare**；
6. 执行器经“update T set value = value+1 where ID =2” 写入到binlog中；
7. 此时提交事务，将redolog中prepare的的记录状态设为**commit**，并且将内存中的新数据刷入磁盘。

##### 为什么要prepare,commit

这种操作 保证以上所有的过程如果出现MySQL实例奔溃都不会导致事务的丢失或异常。

具体分析如下：

* 如果是第5步之前crash，就是还没写任何日志，那么**事务就不存在**，在恢复后从redolog和binlog中都不会有任何的问题；

* 如果写完redolog 的prepare出现了crash，那么恢复时，通过redolog和binlog的对比，会**发现只要一个prepare的日志，那么会将事务进行回滚**，保证redolog和binlog的统一；

* 如果写完binlog后出现crash，那么恢复时，会进行**根据binlog日志对redolog进行补偿**，对redolog之前prepare的记录修改为commit状态，事务得到保证；

* 最后commit后crash，redolog和binlog都是正常的

